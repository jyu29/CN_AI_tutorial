{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_geneeration_rnn_shakespere.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# import package"
      ],
      "metadata": {
        "id": "5jYxCG5heVq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "P2sn6vpjeXZT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Don6W7qeeaMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PIEjUCq4ejuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download text data\n",
        "\n",
        "We need text data to train our model. TensorFlow’s data collection has a text file with contents extracted from various Shakespearean plays. Download the data file from Google Cloud Storage."
      ],
      "metadata": {
        "id": "ZH1XFoZpejTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "file_name= \"shakespeare.txt\"\n",
        " # get the file path\n",
        "path = keras.utils.get_file(file_name, file_URL) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1ffv4ZgeoqE",
        "outputId": "8d2518be-ed68-48de-b769-c1d02889ee56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " raw = open(path, 'rb').read()\n",
        " print(raw[250:400]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE3v-6Fler0j",
        "outputId": "c01163c2-04a2-49e2-aa9b-c802443decfb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " text = raw.decode(encoding='utf-8')\n",
        " print(text[250:400]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZBYMHZEeu9e",
        "outputId": "93f98991-712f-451a-c806-642504a6cbf4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k0TSUBz3exgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorize Word Characters into Integers\n",
        "A deep learning model can not accept text characters as inputs. It should be encoded into integers that a model can understand and process with. Though there are more than a million characters in the given text, there will be a countable number of unique characters. The collection of unique characters is called vocabulary."
      ],
      "metadata": {
        "id": "vPRnw-4ge9Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # unique characters\n",
        " vocabulary = np.array(sorted(set(text)))\n",
        " len(vocabulary) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFmvkZMpfAfH",
        "outputId": "a0950753-3bc5-4cbb-ab09-f6f9002ef45e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bF2NKbU4fF1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a tokenizer that can convert a text character into a corresponding integer. There will be 65 integers starting from 0 and ending at 64. We can assign integers on our own as per the order of characters in the vocabulary."
      ],
      "metadata": {
        "id": "s09hJ2OGfJN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # assign an integer to each character\n",
        " tokenizer = {char:i for i,char in enumerate(vocabulary)} "
      ],
      "metadata": {
        "id": "a_7ZcFcYfL1G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check characters and its corresponding integer\n",
        "for i in range(20):\n",
        "  char = vocabulary[i]\n",
        "  token = tokenizer[char]\n",
        "  print('%4s : %4d'%(repr(char),token)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D80UczsWfPCY",
        "outputId": "43689a4d-ec9c-4914-d651-d75de0695b57"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'\\n' :    0\n",
            " ' ' :    1\n",
            " '!' :    2\n",
            " '$' :    3\n",
            " '&' :    4\n",
            " \"'\" :    5\n",
            " ',' :    6\n",
            " '-' :    7\n",
            " '.' :    8\n",
            " '3' :    9\n",
            " ':' :   10\n",
            " ';' :   11\n",
            " '?' :   12\n",
            " 'A' :   13\n",
            " 'B' :   14\n",
            " 'C' :   15\n",
            " 'D' :   16\n",
            " 'E' :   17\n",
            " 'F' :   18\n",
            " 'G' :   19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ec5neckPfRfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the entire text and check whether the built tokenizer can encode and decode – texts and integers properly. "
      ],
      "metadata": {
        "id": "3WF09uiifgs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = np.array([tokenizer[char] for char in text])\n",
        "print('\\nSample Text \\n')\n",
        "print('-'*70)\n",
        "print(text[:100])\n",
        "print('-'*70)\n",
        "print('\\n\\nCorresponding Integer Vector \\n')\n",
        "print('-'*70)\n",
        "print(vector[:100])\n",
        "print('-'*70) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7-ZV7YffhD_",
        "outputId": "470cbfa8-6911-4f6e-d2c4-700b1a3c0bc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Text \n",
            "\n",
            "----------------------------------------------------------------------\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Corresponding Integer Vector \n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59]\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "08VbBPF8fkq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text with one million encoded characters can not be fed into a model as such. Since we predict characters, the text must be broken down into sequences of some predefined length and then fed into the model. Use TensorFlow’s batch method to create sequences of 100 characters each. Prior to that, convert the NumPy arrays into tensors to make further processes using TensorFlow."
      ],
      "metadata": {
        "id": "UeGx3TZvfyqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert into tensors\n",
        "vector = tf.data.Dataset.from_tensor_slices(vector)\n",
        " # make sequences each of length 100 characters\n",
        "sequences = vector.batch(100, drop_remainder=True) "
      ],
      "metadata": {
        "id": "F6GrIhZHfzNH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sv3OM7DpgAY0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent neural networks predict the subsequent characters based on the past characters. RNNs require a sequence of input characters and the corresponding target sequence with the subsequent characters for training. Prepare input sequences with the first 99 characters and corresponding target sequences with the last 99 characters."
      ],
      "metadata": {
        "id": "O6lRG2jUgHWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def prepare_dataset(seq):\n",
        "     input_vector = seq[:-1]\n",
        "     target_vector = seq[1:]\n",
        "     return input_vector, target_vector\n",
        " dataset = sequences.map(prepare_dataset)"
      ],
      "metadata": {
        "id": "4B_AXRfhgHuA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how it looks\n",
        "for inp, tar in dataset.take(1):\n",
        "  print(inp.numpy())\n",
        "  print(tar.numpy())\n",
        "  inp_text = ''.join(vocabulary[inp])\n",
        "  tar_text = ''.join(vocabulary[tar])\n",
        "  print(repr(inp_text))\n",
        "  print(repr(tar_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXprZgTVgNLJ",
        "outputId": "7190ce5b-71ac-482d-8390-f43991ab7c27"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53]\n",
            "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
            " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
            " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
            "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
            " 37 53 59]\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYo'\n",
            "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JgPYDp31gTTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch and Prefetch data\n",
        "Model will be trained with Stochastic Gradient Descent (SGD) based optimizer Adam. It requires the input data to be batched. Further, TensorFlow’s prefetch method helps training with optimized memory. It fetches data batches just before the training requires them. We prefer not to shuffle the data to retain the contextual order of sequences."
      ],
      "metadata": {
        "id": "CT5jzrixgdcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " AUTOTUNE = tf.data.AUTOTUNE\n",
        " # buffer size 10000\n",
        " # batch size 64\n",
        " data = dataset.batch(64, drop_remainder=True).repeat()\n",
        " data = data.prefetch(AUTOTUNE)\n",
        " # steps per epoch is number of batches available\n",
        " STEPS_PER_EPOCH = len(sequences)//64\n",
        " for inp, tar in data.take(1):\n",
        "     print(inp.numpy().shape)\n",
        "     print(tar.numpy().shape) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLof3qyOgeip",
        "outputId": "bc5007df-d1bc-4e04-c052-21fe6caaaca5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 99)\n",
            "(64, 99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an RNN Model \n",
        "Recurrent neural networks are good at modeling time-dependent data because of its ability to retain time-steps based information in memory. Since texts have contextual information that are determined purely by order of the words, natural language processing heavily relies on sequence modeling architectures such as RNN. Here, an LSTM (Long Short-Term Memory) layers-based recurrent neural network is developed to model the task. While implementing LSTM layers, we enable stateful argument as True to keep the time-step memory of previous states while learning with consequent batches in an epoch. It helps capture the context among consecutive sequences."
      ],
      "metadata": {
        "id": "zKjW0eAIgmqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "     # Embed len(vocabulary) into 64 dimensions\n",
        "     Embedding(len(vocabulary), 64, batch_input_shape=[64,None]),\n",
        "     # LSTM RNN layers\n",
        "     LSTM(512, return_sequences=True, stateful=True),\n",
        "     LSTM(512, return_sequences=True, stateful=True),\n",
        "     # Classification head\n",
        "     Dense(len(vocabulary))\n",
        " ])\n",
        "model.summary() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijqJBqMGguRE",
        "outputId": "1b0c30d2-2261-4b5e-90bc-bf0a93b56ad5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 64)            4160      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 512)           1181696   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 512)           2099200   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            33345     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,318,401\n",
            "Trainable params: 3,318,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lWbFFOD4guON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LBpPxcMfguLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the RNN Model"
      ],
      "metadata": {
        "id": "Aec1bxHyhCAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test whether the untrained model performs good\n",
        "for example_inp, example_tar in data.take(1):\n",
        "  example_pred = model(example_inp)\n",
        "  print(example_tar.numpy().shape)\n",
        "  print(example_pred.shape) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8HVKj8QguH9",
        "outputId": "df2b174b-9c0f-4999-fd1c-fe78d217fdf5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 99)\n",
            "(64, 99, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target shape is (64, 99), which refers to the batch size and the number of characters in that sequence. The last shape, 65, in the prediction refers to the size of the vocabulary. The model predicts the probability of occurrence of each character in the vocabulary. The character with a higher probability has more possibility to be the next character.\n",
        "\n",
        "Compile the model with Adam optimizer and Sparse Categorical Cross-entropy loss function. Since we have not employed softmax as the output layer’s activation function. The outputs will be independent but not mutually exclusive. Hence, we should enable the argument ‘from_logits’ to be True while declaring the loss function. Train the model for 10 epochs."
      ],
      "metadata": {
        "id": "CXzXDATAhO-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', \n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "history = model.fit(data, \n",
        "                     epochs=10, \n",
        "                     steps_per_epoch=STEPS_PER_EPOCH) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0eQfEKdhPbv",
        "outputId": "275bd8a7-229e-4540-fa6b-ef88aa032515"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "174/174 [==============================] - 896s 5s/step - loss: 3.0188\n",
            "Epoch 2/10\n",
            "174/174 [==============================] - 819s 5s/step - loss: 2.2608\n",
            "Epoch 3/10\n",
            "174/174 [==============================] - 825s 5s/step - loss: 1.9833\n",
            "Epoch 4/10\n",
            "174/174 [==============================] - 819s 5s/step - loss: 1.7949\n",
            "Epoch 5/10\n",
            "174/174 [==============================] - 838s 5s/step - loss: 1.6588\n",
            "Epoch 6/10\n",
            "174/174 [==============================] - 842s 5s/step - loss: 1.5628\n",
            "Epoch 7/10\n",
            "174/174 [==============================] - 820s 5s/step - loss: 1.4938\n",
            "Epoch 8/10\n",
            "174/174 [==============================] - 832s 5s/step - loss: 1.4393\n",
            "Epoch 9/10\n",
            "174/174 [==============================] - 844s 5s/step - loss: 1.3954\n",
            "Epoch 10/10\n",
            "174/174 [==============================] - 812s 5s/step - loss: 1.3579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-iCh5i7qhVe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Performance Evaluation\n",
        "Visualizing the losses over epochs may help get better insight on model performance."
      ],
      "metadata": {
        "id": "L4fQnhjNhY_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " plt.plot(history.history['loss'], '+-r')\n",
        " plt.title('Performance Analysis', size=16, color='green')\n",
        " plt.xlabel('Epochs', size=14, color='blue')\n",
        " plt.ylabel('Loss', size=14, color='blue')\n",
        " plt.xticks(range(10))\n",
        " plt.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "LN7CYAawhaIl",
        "outputId": "89e51fe5-a14c-4649-d5d4-37e2d9d4df89"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tOM9W3SUh9p_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference – Next Character Prediction\n",
        "The most awaited part of this task is predicting the next character with the trained model. We can input the model some characters (probably a word) such that it will iteratively predict the next 1000 characters.\n",
        "\n",
        "Before starting prediction with the model, we should reset the model states that were stored in the memory during the last epoch training. However, resetting state memories will not affect the model’s weights."
      ],
      "metadata": {
        "id": "x7_B_dGAh9mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reset previous states of model\n",
        "model.reset_states() "
      ],
      "metadata": {
        "id": "zi4NdE8FiAmr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make predictions by providing the model ‘ANTHONIO:’ as input characters. Nevertheless, the model expects data in three dimensions: the first dimension being the batch size, 64. Vectorize the input characters, expand the dimensions, broadcast the same vector 64 times to obtain a batch of size 64 sequences. Predictions are made based on the logits output by the model. This can be sensitively adjusted by tuning a hyper-parameter called temperature, which refers to the level of randomness in choosing the probable outcome."
      ],
      "metadata": {
        "id": "q4AT1w_ZiLIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By adjusting the temperature value, we can vary randomness and obtain different predictions."
      ],
      "metadata": {
        "id": "CqPGCRD9ierx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'ANTHONIO:'\n",
        " # vectorize the string\n",
        "sample_vector = [tokenizer[s] for s in sample]\n",
        "predicted = sample_vector\n",
        " # convert into tensor of required dimensions\n",
        "sample_tensor = tf.expand_dims(sample_vector, 0) \n",
        " # broadcast to first dimension to 64 \n",
        "sample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n",
        "\n",
        " # predict next 1000 characters\n",
        " # vary temperature to change randomness\n",
        "temperature = 0.8\n",
        "\n",
        "for i in range(1000):\n",
        "     pred = model(sample_tensor)\n",
        "     # reduce unnecessary dimensions\n",
        "     pred = pred[0].numpy()/temperature\n",
        "     pred = tf.random.categorical(pred, num_samples=1)[-1,0].numpy()\n",
        "     predicted.append(pred)\n",
        "     sample_tensor = predicted[-99:]\n",
        "     sample_tensor = tf.expand_dims([pred],0)\n",
        "     # broadcast to first dimension to 64 \n",
        "     sample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n",
        "\n",
        " # integer to text decoding\n",
        "pred_char = [vocabulary[i] for i in predicted]\n",
        "generated = ''.join(pred_char)\n",
        "print(generated) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy90qdgzifFO",
        "outputId": "58178743-815b-418d-fdff-0b1471915af9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANTHONIO:\n",
            "Be pray, sir, call us prepared with her.\n",
            "\n",
            "PROSPERO:\n",
            "Ay, in them are, if I come.\n",
            "\n",
            "BIONDELLO:\n",
            "What. how now, she be known be hanged\n",
            "And\n",
            "great the prince of heavens and all this spire,\n",
            "The curst thy deverence, and with goods?\n",
            "\n",
            "ABLO:\n",
            "Cannot so: it is a thousand passing\n",
            "And see them not! This is the words thou liest,\n",
            "Sweet without your hands: the widest consented fill thou must\n",
            "she kind reposed in masters, bidts forst and Vaint on thee;\n",
            "To say Vercease your blood with thy\n",
            "All as hust thou did'st rett the other love.\n",
            "\n",
            "BIONDELLO:\n",
            "He'r well have scured and mine more weep,\n",
            "But then awake.\n",
            "\n",
            "PETRUCHIO:\n",
            "God since my masters behold no langer on the satistake,\n",
            "And Baptista's sister, say the choler certage\n",
            "As is the dielderch saints in histands tongue\n",
            "Hay the duke is his controat is not some pope men that think,\n",
            "Hents thou confess?\n",
            "\n",
            "BIONDELLO:\n",
            "The time head in your shame!\n",
            "\n",
            "GRUMIO:\n",
            "What man nate, be sit. Tonother! sir!\n",
            "Farewell and this of the duke.\n",
            "\n",
            "PETRUCHIO:\n",
            "Some all the masure sold these night re\n"
          ]
        }
      ]
    }
  ]
}